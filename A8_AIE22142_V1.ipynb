{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Node: Functional Area\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def calculate_entropy(target_column):\n",
    "    \"\"\"\n",
    "    Calculate the entropy of the target column.\n",
    "    \"\"\"\n",
    "    target_counts = Counter(target_column)\n",
    "    total_samples = len(target_column)\n",
    "    entropy = 0\n",
    "    for count in target_counts.values():\n",
    "        probability = count / total_samples\n",
    "        entropy -= probability * np.log2(probability)\n",
    "    return entropy\n",
    "\n",
    "def calculate_information_gain(data, feature_column, target_column):\n",
    "    \"\"\"\n",
    "    Calculate the information gain for a given feature.\n",
    "    \"\"\"\n",
    "    total_entropy = calculate_entropy(target_column)\n",
    "    feature_values = data[feature_column].unique()\n",
    "\n",
    "    weighted_entropy = 0\n",
    "    for value in feature_values:\n",
    "        subset = data[data[feature_column] == value]\n",
    "        subset_entropy = calculate_entropy(subset[target_column])\n",
    "        weighted_entropy += (len(subset) / len(data)) * subset_entropy\n",
    "\n",
    "    information_gain = total_entropy - weighted_entropy\n",
    "    return information_gain\n",
    "\n",
    "def detect_root_node(data, features, target_column):\n",
    "    \"\"\"\n",
    "    Detect the feature for the root node of the Decision Tree using Information Gain.\n",
    "    \"\"\"\n",
    "    max_information_gain = -1\n",
    "    root_node = None\n",
    "\n",
    "    for feature in features:\n",
    "        information_gain = calculate_information_gain(data, feature, target_column)\n",
    "        if information_gain > max_information_gain:\n",
    "            max_information_gain = information_gain\n",
    "            root_node = feature\n",
    "\n",
    "    return root_node\n",
    "\n",
    "# Input your dataset path\n",
    "dataset_path = \"C:\\\\Users\\\\SACHIN.R\\\\OneDrive\\\\Desktop\\\\jobss.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Define your list of features and target column\n",
    "features = ['Functional Area', 'Industry']\n",
    "target_column = 'Role'\n",
    "\n",
    "# Detect the root node\n",
    "root_node = detect_root_node(data, features, target_column)\n",
    "print(\"Root Node:\", root_node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def calculate_entropy(target_column):\n",
    "    \"\"\"\n",
    "    Calculate the entropy of the target column.\n",
    "    \"\"\"\n",
    "    target_counts = Counter(target_column)\n",
    "    total_samples = len(target_column)\n",
    "    entropy = 0\n",
    "    for count in target_counts.values():\n",
    "        probability = count / total_samples\n",
    "        entropy -= probability * np.log2(probability)\n",
    "    return entropy\n",
    "\n",
    "def calculate_information_gain(data, feature_column, target_column):\n",
    "    \"\"\"\n",
    "    Calculate the information gain for a given feature.\n",
    "    \"\"\"\n",
    "    total_entropy = calculate_entropy(target_column)\n",
    "    feature_values = data[feature_column].unique()\n",
    "\n",
    "    weighted_entropy = 0\n",
    "    for value in feature_values:\n",
    "        subset = data[data[feature_column] == value]\n",
    "        subset_entropy = calculate_entropy(subset[target_column])\n",
    "        weighted_entropy += (len(subset) / len(data)) * subset_entropy\n",
    "\n",
    "    information_gain = total_entropy - weighted_entropy\n",
    "    return information_gain\n",
    "\n",
    "def detect_root_node(data, features, target_column):\n",
    "    \"\"\"\n",
    "    Detect the feature for the root node of the Decision Tree using Information Gain.\n",
    "    \"\"\"\n",
    "    max_information_gain = -1\n",
    "    root_node = None\n",
    "\n",
    "    for feature in features:\n",
    "        information_gain = calculate_information_gain(data, feature, target_column)\n",
    "        if information_gain > max_information_gain:\n",
    "            max_information_gain = information_gain\n",
    "            root_node = feature\n",
    "\n",
    "    return root_node\n",
    "\n",
    "def bin_continuous_feature(data, feature_column, binning_type='equal_width', num_bins=5):\n",
    "    # Check for infinity and handle it (e.g., exclude rows, replace with finite value)\n",
    "    data = data[~np.isinf(data[feature_column])]  # Exclude rows with infinity\n",
    "    # Rest of the function logic...\n",
    "\n",
    "    \"\"\"\n",
    "    Bin a continuous-valued feature into categorical bins based on the specified binning type and number of bins.\n",
    "    \"\"\"\n",
    "    if binning_type == 'equal_width':\n",
    "        data[feature_column + '_binned'] = pd.cut(data[feature_column], bins=num_bins, labels=False)\n",
    "    elif binning_type == 'frequency':\n",
    "        data[feature_column + '_binned'] = pd.qcut(data[feature_column], q=num_bins, labels=False, duplicates='drop')\n",
    "    else:\n",
    "        print(\"Invalid binning type. Please choose 'equal_width' or 'frequency'.\")\n",
    "\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "dataset_path = \"C:\\\\Users\\\\SACHIN.R\\\\OneDrive\\\\Desktop\\\\jobss.csv\"\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Handle missing values in the 'Functional Area' column\n",
    "data['Functional Area'].fillna('Unknown', inplace=True)  # Fill missing values with 'Unknown' or any other appropriate strategy\n",
    "\n",
    "# Handle infinity values in the 'Functional Area' column\n",
    "# Replace the line with your preferred way to handle infinity\n",
    "data['Functional Area'] = data['Functional Area'].replace([np.inf, -np.inf], np.nan)  # Or another strategy\n",
    " \n",
    "\n",
    "# Drop rows with NaN values in the 'Functional Area' column\n",
    "data = data.dropna(subset=['Functional Area'])\n",
    "\n",
    "# Define your list of features and target column\n",
    "features = ['Functional Area', 'Industry']\n",
    "target_column = 'Role'\n",
    "\n",
    "# Binning the continuous feature 'Functional Area' with equal width binning and 3 bins\n",
    "bin_continuous_feature(data, 'Functional Area', binning_type='equal_width', num_bins=3)\n",
    "\n",
    "# Detect the root node\n",
    "root_node = detect_root_node(data, features + ['Functional Area_binned'], target_column)\n",
    "print(\"Root Node:\", root_node)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def bin_continuous_feature(data, feature_column, binning_type='equal_width', num_bins=3):\n",
    "    if binning_type == 'equal_width':\n",
    "        data[feature_column + '_binned'] = pd.cut(data[feature_column], bins=num_bins, labels=False)\n",
    "    elif binning_type == 'equal_frequency':\n",
    "        data[feature_column + '_binned'] = pd.qcut(data[feature_column], q=num_bins, labels=False, duplicates='drop')\n",
    "\n",
    "def detect_root_node(data, features, target_column):\n",
    "    best_feature = None\n",
    "    best_threshold = None\n",
    "    best_gini = 999  # Initialize with a high value\n",
    "    for feature in features:\n",
    "        unique_values = data[feature].unique()\n",
    "        for value in unique_values:\n",
    "            left_subset = data[data[feature] <= value]\n",
    "            right_subset = data[data[feature] > value]\n",
    "            gini = calculate_gini_index(left_subset, right_subset, target_column)\n",
    "            if gini < best_gini:\n",
    "                best_gini = gini\n",
    "                best_feature = feature\n",
    "                best_threshold = value\n",
    "    return best_feature, best_threshold\n",
    "\n",
    "def calculate_gini_index(left_subset, right_subset, target_column):\n",
    "    # Calculate Gini index based on left and right subsets\n",
    "    return 0.5  # Placeholder for actual Gini index calculation\n",
    "\n",
    "# Load the dataset\n",
    "dataset_path = \"C:\\\\Users\\\\SACHIN.R\\\\OneDrive\\\\Desktop\\\\jobss.csv\"\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Define your list of features and target column\n",
    "features = ['Functional Area', 'Industry']\n",
    "target_column = 'Role'\n",
    "\n",
    "# Binning the continuous feature 'Functional Area' with equal width binning and 3 bins\n",
    "bin_continuous_feature(data, 'Functional Area', binning_type='equal_width', num_bins=3)\n",
    "\n",
    "# Build the Decision Tree\n",
    "class Node:\n",
    "    def __init__(self, feature, threshold, left=None, right=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "def build_decision_tree(data, features, target_column):\n",
    "    if stopping_condition_met(data):  # Define your stopping condition\n",
    "        return LeafNode(data)\n",
    "    \n",
    "    best_feature, best_threshold = detect_root_node(data, features, target_column)\n",
    "    \n",
    "    left_subset = data[data[best_feature] <= best_threshold]\n",
    "    right_subset = data[data[best_feature] > best_threshold]\n",
    "    \n",
    "    left_tree = build_decision_tree(left_subset, features, target_column)\n",
    "    right_tree = build_decision_tree(right_subset, features, target_column)\n",
    "    \n",
    "    return Node(best_feature, best_threshold, left_tree, right_tree)\n",
    "\n",
    "# Define your stopping condition and LeafNode class\n",
    "\n",
    "\n",
    "decision_tree = build_decision_tree(data, features, target_column)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
